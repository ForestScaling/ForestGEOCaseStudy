---
title: "Supplementary Material Case Study"
author: "Adam Eichenwald"
date: "2024-07-24"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Overview

In this supplementary document, we explain the process to replicate our case study. This includes creating the random forest model, using it to predict $\alpha$ from the power law distribution, estimating the total number of trees at a given site, and finally obtaining the final size-abundance distribution for a given area.

The packages we used are:
```{r, message=FALSE}
library(tidyverse)
library(data.table)
library(terra)
library(smplot2)
library(sf)
library(rstan)
library(smplot2)
library(EnvStats)
library(VGAM)
library(rstantools)
library(ggplot2)
library(mapview)
library(forestscaling)
library(Rmisc)
library(EnvStats)
library(geodata)
library(terra)
library(caret)
library(caTools)
library(ranger)
library(itcSegment)
library(webshot)
library(posterior)
library(rFIA) # must be downloaded from GitHub directly
# webshot::install_phantomjs()
summarize<-dplyr::summarize
filter<-dplyr::filter
select<-dplyr::select
rename<-dplyr::rename

```

#### Obtaining Data
Data used in this study are publicly available. We are not able to provide the data ourselves, but a reader can obtain this data directly from the FIA and ForestGEO projects.

We provided code to download FIA data in an earlier R file. However, readers must go directly to the ForestGEO data portal to download HARV and SERC data. Those two sites are relatively easy to download, and generally provide instant approval.

ForestGEO data portal: http://ctfs.si.edu/datarequest/

#### Random Forest Model
We first calculate the random forest model that we will use to predict $\alpha$ for the ForestGEO plots from environmental variables. We are estimating the model using data gathered from the FIA program. We first split our data into training and testing subsections.

```{r}
rdata<-fread("rangerrdata.csv")
set.seed(101) 

```

Then we fit the model. Included variables are maximum height, stand age, longitude, latitude, proportion of hardwood trees in the site, total annual precipitation, elevation, and average annual temperature.

```{r}

rangermodel<-ranger(alpha~Max_Height+ STDAGE+ LON+LAT+Prop_Hardwood_to_Softwood+
       Total_precip+elevation+avgtemp, num.threads = 11,
       data= rdata%>%
         drop_na(), keep.inbag = TRUE,
       write.forest = TRUE, num.trees = 5000, verbose= TRUE)
```

#### Environmental Variable Setup
We obtain rasterized environmental variables from which to extract data about our ForestGEO plots.
```{r}
#Download monthly temperature data
bioclim_tavg <- worldclim_country(country = "United States of America",
                                  var = c("tavg"),
                                 res = 0.5,
                                 path = "data/")
#Download monthly precipitation data
bioclim_prec<-worldclim_country(country = "United States of America",
                                var = c("prec"),
                               res = 0.5,
                               path = "data/")
#Calculate average temperature in a year
bioclim_tavgmean<-mean(bioclim_tavg)
#Calculate total precipitation that falls within a year
bioclim_precsum<-sum(bioclim_prec)
#Read in the stand age map interpolated through Google Earth Engine (see other supplemental material)
standagemap<-rast("standagemapsmaller.tif")
```

FIA also classifies species by whether they are hardwood or softwood. We include the proportion of hardwood to softwood in the random forest model, so we need to also make sure we can classify the tree crown shapefile by joining them to FIA designations. We prep that here.
```{r}
species<-fread("C://Users//adam.local//Downloads//v9-2_2022-09_Natl_MasterTreeSpeciesList.csv")
species<-species%>%
  rename(SPCD = `FIA Code`)
speciesgroup<-fread("FIAspeciesandgroups.csv")
speciesall<-species%>%
  inner_join(speciesgroup)%>%
  select(Genus,Species,SPGRPCD)

```

#### Harvard Forest Case Study
Our first case study recovering size distributions of forests from remote sensing data comes from the Harvard Forest in MA. First, we read in our field data and our remotely sensed crown data of the plot.

```{r}
HarvFieldData<-fread("C:\\Users\\adam.local\\Documents\\HarvFieldData.csv")%>%
  data.frame()
HarvFieldData<-st_as_sf(HarvFieldData%>%
           separate(geometry,sep = "\\|", 
                    into =c("x","y")),
         coords = c("x","y"),crs=32618)
(head(HarvFieldData))

bencrown<-read_sf("C:\\Users\\adam.local\\Documents\\HarvForestGeo.shp")%>%
  st_set_crs(32618)

#Bring in elevation data for Harvard Forest
Harvelev<-terra::rast("NEON_elevation\\NEON_D01_HARV_DP3_731000_4713000_DTM.tif")
Harvelev_1<-terra::rast("NEON_elevation\\NEON_D01_HARV_DP3_732000_4713000_DTM.tif")
mosaicHarvelev<-mosaic(Harvelev, Harvelev_1)


```

Tree abundance in size distributions are generally measured in "trees per area" (often hectares). Therefore, we split the ForestGEO site into multiple smaller sites of 1 hectare each. We make sure both the field data and the crown delineation maps are assigned to 1 hectare plots correctly.


```{r}
# Define bounding box
g_bbox <- st_bbox(c(xmin = 731592, 
                    ymin = 4713222, 
                    xmax = 732295, 
                    ymax = 4713725), crs = 32618)

# Convert bounding box to polygon and transform
g_poly <- st_as_sfc(g_bbox) %>%
  st_transform(32618)


# Create grid within the polygon
g_grid <- st_make_grid(g_poly, units::as_units(10000, "m^2"))#cellsize = 150 / sqrt(2))

# Assign unique IDs to the plots
plot_ids <- paste0("plot_", seq_len(length(g_grid)))

# Convert grid to sf object
plots <- st_as_sf(g_grid) %>%
  mutate(plot_id = plot_ids)


# Spatial join between HarvFieldData and plots.
HarvFieldfragmented <- st_join(HarvFieldData, plots)
mapview::mapview(plots, legend = FALSE)

bencrownfragmented <- st_join(bencrown, plots, largest = TRUE)

```

The canopy height model is prepped by NEON, but it's available in two different raster files. We mosaic them together.

```{r}
Harv2019<-terra::rast("C:\\Users\\adam.local\\Downloads\\NEON_D01_HARV_DP3_731000_4713000_CHM.tif")
Harv2019_1<-terra::rast("C:\\Users\\adam.local\\Downloads\\harvfield\\NEON_D01_HARV_DP3_732000_4713000_CHM.tif")
mosaicHarv<-mosaic(Harv2019, Harv2019_1)

print(mosaicHarv)

```

The ForestGEO site is much smaller than the coverage of the NEON site, so the mosaic is larger than we need. We clip it to the right size.

```{r}
e<-ext(bencrown)
mosaic2<-terra::crop(mosaicHarv,e)
mosaic2<-terra::rast("Harvheight.tif")
mapview::mapview(mosaic2)
```

Recover DBH from the canopy height raster for all trees from the crown map.

```{r}
bencrownfragmented$perimeter<-as.numeric(st_perimeter(bencrownfragmented))
bencrownfragmented$area<-as.numeric(st_area(bencrownfragmented))
# Extract maximum height values from 'mosaic2' using 'bencrown'
benheightfragmented <- terra::extract(mosaic2, bencrownfragmented, fun = max) %>%
  
  # Rename the extracted height column
    rename(Height = b1) %>%

  # Combine with 'bencrown'
  cbind(bencrownfragmented) %>%
  
  # Filter out heights that are equal to 0, since that's just missing data
  filter(Height != 0) %>%
  
  # Extract just the scientific name from 'sci_name', currently looks like "Tsuga canadensis (L.) CarriÃ¨re"
  mutate(sci_name = str_extract(sci_name, "\\b\\w+\\b \\b\\w+\\b")) %>%
  data.frame()%>%
  # Calculate Diameter from crown area.
  dplyr::mutate(Diameter = 0.5*(sqrt(perimeter^2-(8*area)))) %>%
  # Calculate dbh
  mutate(dbh = dbh(H=Height, CA=Diameter))


```




```{r}
# Initialize empty list to store plot data
plotdata <- list()

# Transform plot coordinates to WGS84, get centroids, and combine with plot data
plotswgs <- st_centroid(plots %>% st_transform("WGS84")) %>% 
            st_coordinates() %>%
            cbind(plots)

# Initialize empty list to store new data
newdata1 <- list()

# Project mosaicHarvelev to WGS84
mosaicHarvelev <- project(mosaicHarvelev, "WGS84")
bencrownfragmented<-vect(bencrownfragmented)%>%
project("WGS84")
```

Loop through unique plot IDs in HarvFieldfragmented and get all the environmental variables for each 1 hectare plot.
```{r, warning=FALSE}
for (i in na.omit(unique(HarvFieldfragmented$plot_id))) {
  # Subset bencrownfragmented for current plot ID
  bencrownsubset <- subset(bencrownfragmented, bencrownfragmented$plot_id == i)
  
  # Create new data frame with expanded grid of Max_Height and other predictors
  newdata <- expand_grid(
    Max_Height = (benheightfragmented %>%
                    filter(plot_id == i) %>%
                    #Height is given in meters in the raster file,
                    # but FIA calculates it in feet
                    summarize(maxht = max(Height)))$maxht * 3.281,
    #Stand age is not calculated at a fine scale in our available raster map. We buffer around each plot for about 1 km to get a sense for forest age at a coarse level.
    STDAGE = terra::extract(standagemap, buffer(vect(plots %>% filter(plot_id == i)), width = 1000), fun = "mean")[,2],
    
    Prop_Hardwood_to_Softwood = (benheightfragmented %>%
                                   filter(plot_id == i) %>%
                                   separate(sci_name, into = c("Genus", "Species")) %>%
                                   left_join(speciesall %>%
                                               select(Genus, Species, SPGRPCD) %>%
                                               distinct(Genus, Species, .keep_all = TRUE)) %>%
                                   drop_na(SPGRPCD) %>%
                                   mutate(Forest_type = ifelse(SPGRPCD >= 1 & SPGRPCD <= 24, "Softwood", "Placeholder")) %>%
                                   mutate(Forest_type = ifelse(SPGRPCD >= 25 & SPGRPCD <= 48, "Hardwood", Forest_type)) %>%
                                   mutate(Forest_type = ifelse(SPGRPCD == 55 | SPGRPCD == 53 | SPGRPCD == 54, "Hardwood", Forest_type)) %>%
                                   mutate(Forest_type = ifelse(SPGRPCD == 56 | SPGRPCD == 51 | SPGRPCD == 52, "Softwood", Forest_type)) %>%
                                   group_by(Forest_type) %>%
                                   summarize(Tree_number = n()) %>%
                                   ungroup() %>%
                                   pivot_wider(names_from = "Forest_type", values_from = "Tree_number") %>%
                                   mutate(Proportion = ifelse(!("Hardwood" %in% names(.)), 0,
                                                              ifelse(!("Softwood" %in% names(.)), 1,
                                                                     Hardwood / (Hardwood + Softwood)))))$Proportion,
    avgtemp = terra::extract(bioclim_tavgmean, vect(ext(bencrownsubset)), fun = "mean")$mean,
    Total_precip = terra::extract(bioclim_precsum, vect(ext(bencrownsubset)), fun = "mean")$sum,
    elevation = terra::extract(mosaicHarvelev, vect(ext(bencrownsubset)), fun = "mean")[1,2]) %>%
  mutate(LAT = (plotswgs %>% filter(plot_id == i))$Y,
         LON = (plotswgs %>% filter(plot_id == i))$X)
  
  # Add new data to the list
  newdata1[[length(newdata1) + 1]] <- newdata %>%
    mutate(plot_id = i)
}
```

Now we predict alpha with the random forest model, making sure to carry uncertainty through by calculating CIs.
```{r, warning=FALSE}
# Predict standard error using the random forest model
dpredictse <- predict(rangermodel, newdata1 %>% rbindlist(), type = "se")

# Create a data frame with predicted alpha_mean and standard error
plotdata <- data.frame(
  alpha_mean = dpredictse$predictions,
  plot_id = (unique(newdata1 %>% rbindlist() %>% select(plot_id)))$plot_id,
  se = dpredictse$se
)

# Calculate confidence intervals
plotdata <- plotdata %>%
  mutate(CIhigh = alpha_mean + 1.96 * se,
         CIlow = alpha_mean - 1.96 * se)
```

Then we predict what the size distribution should look like based on the different estimates for alpha, and we also estimate how many trees there are in the plot.
```{r}
# Create a new data frame for x values
new_xs <- data.frame(x = seq(from = log10(3), to = log10(50), length.out = 50))

# Initialize lists for plot data, remote data, and predictions
plotdata1 <- list()
remote_data <- list()
predictions <- list()

# Loop through specified sequence and plot data to fit models and make predictions
for (i in seq(round(min(benheightfragmented$dbh)), 46, 5)) {
  for (q in 1:nrow(plotdata)) {
    plot <- (plotdata[q,])$plot_id
    j <- (plotdata[q,])$alpha_mean
    
    x_values <- seq(i, 50, length.out = 500)
    mod <- lm(y ~ x,
              data.frame(x = log10(x_values),
                         #this is where we estimate the number of trees in the plot
                         y = log10(dtruncpareto(x_values, lower = i, upper = 50, shape = j) * nrow(benheightfragmented %>%
                                                                                                  filter(plot_id == plot) %>%
                                                                                                  filter(dbh <= 50 & dbh >= i)))))
    predictions <- predict(mod, newdata = new_xs)
    
    Ntot_estimated_draws <- sm_auc(x = 10^new_xs$x, y = 10^predictions)
    remote_data[[length(remote_data) + 1]] <- data.frame(
      x = 10^new_xs$x, y = 10^predictions, shape = j,
      Ntot_estimated_draws = Ntot_estimated_draws, multbayesnumber = i, plot_id = plot, alpha = "mean"
    )
    
    j <- (plotdata[q,])$CIhigh
    x_values <- seq(i, 50, length.out = 500)
    mod <- lm(y ~ x,
              data.frame(x = log10(x_values),
                         y = log10(dtruncpareto(x_values, lower = i, upper = 50, shape = j) * nrow(benheightfragmented %>%
                                                                                                  filter(plot_id == plot) %>%
                                                                                                  filter(dbh <= 50 & dbh >= i)))))
    predictions <- predict(mod, newdata = new_xs)
    
    Ntot_estimated_draws <- sm_auc(x = 10^new_xs$x, y = 10^predictions)
    remote_data[[length(remote_data) + 1]] <- data.frame(
      x = 10^new_xs$x, y = 10^predictions, shape = j,
      Ntot_estimated_draws = Ntot_estimated_draws, multbayesnumber = i, plot_id = plot, alpha = "CIhigh"
    )
    
    j <- (plotdata[q,])$CIlow
    x_values <- seq(i, 50, length.out = 500)
    mod <- lm(y ~ x,
              data.frame(x = log10(x_values),
                         y = log10(dtruncpareto(x_values, lower = i, upper = 50, shape = j) * nrow(benheightfragmented %>%
                                                                                                  filter(plot_id == plot) %>%
                                                                                                  filter(dbh <= 50 & dbh >= i)))))
    predictions <- predict(mod, newdata = new_xs)
    
    Ntot_estimated_draws <- sm_auc(x = 10^new_xs$x, y = 10^predictions)
    remote_data[[length(remote_data) + 1]] <- data.frame(
      x = 10^new_xs$x, y = 10^predictions, shape = j,
      Ntot_estimated_draws = Ntot_estimated_draws, multbayesnumber = i, plot_id = plot, alpha = "CIlow"
    )
  }
}
```

Combine remote data into a single data frame and label it.

```{r}
remote_data <- remote_data %>%
  rbindlist() %>%
  mutate(Label = "Remote")

```

Now we estimate the size distribution from the survey data, to have the real world data to compare with.

```{r}
fieldpareto<-list()
for (i in na.omit(unique(HarvFieldfragmented$plot_id))){
test<-HarvFieldfragmented%>%
  filter(plot_id == i)%>%
  filter(dbh > 3 &dbh<50)

# Number of observations
N <- length(test$dbh)

# Extract diameters
x <- test$dbh

# Minimum diameter
x_min <- 3

# Prepare data for Stan model
stan_dat <- list(N = N, x = x, x_min = x_min)
model <- stan_model(file = "C:\\Users\\adam.local\\Downloads\\density1_simplified.stan")
select<-dplyr::select
# Run Stan model
fit <- sampling(model, data = stan_dat, iter = 9000, warmup = 6000)

#Return alpha
fieldpareto[[length(fieldpareto)+1]]<-data.frame(alpha_draws=as_draws_df(fit, variable = "alpha")$alpha,
                                                 alpha_mean=(summarize_draws(fit)%>%
                                                               filter(variable == "alpha"))$mean,
                                                 plot_id = i,
                                                 Label = "Field")%>%
  distinct(alpha_mean, plot_id, .keep_all = TRUE)
}

#Combine alphas into a single data frame
fieldpareto<-fieldpareto%>%
  rbindlist()
```

With alphas estimated, we multiply by the total number of trees in the plot from 3-50cm so that we have a true size-abundance relationship instead of a size-density relationship.

```{r}
field_se<-list()
for (j in unique(fieldpareto$plot_id)){
  n<-(fieldpareto%>%
    filter(plot_id==j))$alpha_mean
  x_values<-seq(3, 50, length.out = 500)
  mod<-lm(y~x,
          data.frame(x=log10(x_values),
                     y=log10(dtruncpareto(x_values,
                                          lower = 3, upper = 50, shape = n)*
                               nrow(HarvFieldfragmented%>%
                                      filter(plot_id == j)%>%
                                      filter(dbh <=50 & dbh >= 3)))))
  predictions <- predict(mod, newdata = new_xs)
  # Return a data frame with predictions
  field_se[[length(field_se)+1]]<-data.frame(x = 10^new_xs$x, y= 10^predictions,
                                             alpha_mean = n, plot_id = j)
  
}


field_se<-field_se%>%
  rbindlist()
```

Finally, with estimates from both the field and remote sensing data, we run a bayesian model to see whether they provide similar results.

```{r}

multiplebayes<-list()

for (i in unique(remote_data$multbayesnumber)){
  for(j in unique(remote_data$alpha)){
  multiplebayes[[length(multiplebayes)+1]]<-remote_data%>%
    filter(multbayesnumber==i)%>%
    filter(alpha == j)%>%
    mutate(Label = "Remote")%>%
    select(x,y,Label,plot_id)%>%
    mutate(x=log10(x),
           y=log10(y))%>%
    rbind(field_se%>%
            select(-alpha_mean)%>%
            mutate(Label = "Field")%>%
            mutate(x=log10(x),
                   y=log10(y)))
  }
}
```
That code created a list of data frames where alpha and number of trees in the plot are estimated differently. By fitting brm_multiple() to all of these data frames together in the same function, we are able to propagate uncertainty through.


```{r}
bayesianformula<-bf(y~ Label*x+(1|plot_id))
bmod<-brm_multiple(bayesianformula,
                   data=multiplebayes, iter = 9000, warmup = 6000,
                   chains = 4, cores=4,sample_prior = TRUE)
c_eff<-conditional_effects(bmod, "x:Label")
plot(c_eff, plot = FALSE)[[1]]
```

Finally, we look at the region of practical equivalence (ROPE), which will tell us if there is a relevant difference between the remote sensing and field data estimations.

```{r}
rope(bmod)
```

